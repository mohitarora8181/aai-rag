{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-02-08T13:58:14.938137Z",
     "iopub.status.busy": "2025-02-08T13:58:14.937838Z",
     "iopub.status.idle": "2025-02-08T13:58:14.944583Z",
     "shell.execute_reply": "2025-02-08T13:58:14.943736Z",
     "shell.execute_reply.started": "2025-02-08T13:58:14.938114Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '.venv (Python 3.10.10)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/usar/Documents/work/aai/.venv/Scripts/python.exe -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "    # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_kg_hide-output": false,
    "execution": {
     "iopub.execute_input": "2025-02-08T13:58:17.395991Z",
     "iopub.status.busy": "2025-02-08T13:58:17.395710Z",
     "iopub.status.idle": "2025-02-08T13:58:27.825937Z",
     "shell.execute_reply": "2025-02-08T13:58:27.825075Z",
     "shell.execute_reply.started": "2025-02-08T13:58:17.395968Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade langchain\n",
    "!pip install -U langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T13:58:27.827483Z",
     "iopub.status.busy": "2025-02-08T13:58:27.827116Z",
     "iopub.status.idle": "2025-02-08T13:58:31.049348Z",
     "shell.execute_reply": "2025-02-08T13:58:31.048503Z",
     "shell.execute_reply.started": "2025-02-08T13:58:27.827456Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting streamlit\n",
      "  Downloading streamlit-1.42.0-py2.py3-none-any.whl (9.6 MB)\n",
      "     ---------------------------------------- 9.6/9.6 MB 11.3 MB/s eta 0:00:00\n",
      "Collecting toml<2,>=0.10.1\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Collecting rich<14,>=10.14.0\n",
      "  Using cached rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "Requirement already satisfied: numpy<3,>=1.23 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from streamlit) (1.26.4)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4.0 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from streamlit) (4.12.2)\n",
      "Collecting blinker<2,>=1.0.0\n",
      "  Using cached blinker-1.9.0-py3-none-any.whl (8.5 kB)\n",
      "Requirement already satisfied: packaging<25,>=20 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from streamlit) (24.2)\n",
      "Collecting pandas<3,>=1.4.0\n",
      "  Using cached pandas-2.2.3-cp310-cp310-win_amd64.whl (11.6 MB)\n",
      "Collecting click<9,>=7.0\n",
      "  Using cached click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Collecting altair<6,>=4.0\n",
      "  Downloading altair-5.5.0-py3-none-any.whl (731 kB)\n",
      "     ------------------------------------- 731.2/731.2 kB 15.3 MB/s eta 0:00:00\n",
      "Collecting pyarrow>=7.0\n",
      "  Downloading pyarrow-19.0.0-cp310-cp310-win_amd64.whl (25.3 MB)\n",
      "     --------------------------------------- 25.3/25.3 MB 10.9 MB/s eta 0:00:00\n",
      "Collecting protobuf<6,>=3.20\n",
      "  Using cached protobuf-5.29.3-cp310-abi3-win_amd64.whl (434 kB)\n",
      "Requirement already satisfied: tenacity<10,>=8.1.0 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from streamlit) (9.0.0)\n",
      "Requirement already satisfied: requests<3,>=2.27 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from streamlit) (2.32.3)\n",
      "Collecting cachetools<6,>=4.0\n",
      "  Using cached cachetools-5.5.1-py3-none-any.whl (9.5 kB)\n",
      "Collecting watchdog<7,>=2.1.5\n",
      "  Downloading watchdog-6.0.0-py3-none-win_amd64.whl (79 kB)\n",
      "     ---------------------------------------- 79.1/79.1 kB ? eta 0:00:00\n",
      "Collecting gitpython!=3.1.19,<4,>=3.0.7\n",
      "  Downloading GitPython-3.1.44-py3-none-any.whl (207 kB)\n",
      "     ------------------------------------- 207.6/207.6 kB 13.2 MB/s eta 0:00:00\n",
      "Collecting pillow<12,>=7.1.0\n",
      "  Using cached pillow-11.1.0-cp310-cp310-win_amd64.whl (2.6 MB)\n",
      "Collecting pydeck<1,>=0.8.0b4\n",
      "  Downloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
      "     ---------------------------------------- 6.9/6.9 MB 11.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from streamlit) (6.4.2)\n",
      "Collecting narwhals>=1.14.2\n",
      "  Downloading narwhals-1.25.2-py3-none-any.whl (305 kB)\n",
      "     ------------------------------------- 305.5/305.5 kB 18.5 MB/s eta 0:00:00\n",
      "Collecting jsonschema>=3.0\n",
      "  Downloading jsonschema-4.23.0-py3-none-any.whl (88 kB)\n",
      "     ---------------------------------------- 88.5/88.5 kB 4.9 MB/s eta 0:00:00\n",
      "Collecting jinja2\n",
      "  Using cached jinja2-3.1.5-py3-none-any.whl (134 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from click<9,>=7.0->streamlit) (0.4.6)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Downloading gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "     ---------------------------------------- 62.8/62.8 kB 3.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1\n",
      "  Downloading pytz-2025.1-py2.py3-none-any.whl (507 kB)\n",
      "     -------------------------------------- 507.9/507.9 kB 8.0 MB/s eta 0:00:00\n",
      "Collecting tzdata>=2022.7\n",
      "  Using cached tzdata-2025.1-py2.py3-none-any.whl (346 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from requests<3,>=2.27->streamlit) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from requests<3,>=2.27->streamlit) (2025.1.31)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from requests<3,>=2.27->streamlit) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from rich<14,>=10.14.0->streamlit) (2.19.1)\n",
      "Collecting markdown-it-py>=2.2.0\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Collecting smmap<6,>=3.0.1\n",
      "  Downloading smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Collecting MarkupSafe>=2.0\n",
      "  Using cached MarkupSafe-3.0.2-cp310-cp310-win_amd64.whl (15 kB)\n",
      "Collecting referencing>=0.28.4\n",
      "  Downloading referencing-0.36.2-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.1.0)\n",
      "Collecting rpds-py>=0.7.1\n",
      "  Downloading rpds_py-0.22.3-cp310-cp310-win_amd64.whl (231 kB)\n",
      "     -------------------------------------- 231.7/231.7 kB 6.9 MB/s eta 0:00:00\n",
      "Collecting jsonschema-specifications>=2023.03.6\n",
      "  Downloading jsonschema_specifications-2024.10.1-py3-none-any.whl (18 kB)\n",
      "Collecting mdurl~=0.1\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
      "Installing collected packages: pytz, watchdog, tzdata, toml, smmap, rpds-py, pyarrow, protobuf, pillow, narwhals, mdurl, MarkupSafe, click, cachetools, blinker, referencing, pandas, markdown-it-py, jinja2, gitdb, rich, pydeck, jsonschema-specifications, gitpython, jsonschema, altair, streamlit\n",
      "Successfully installed MarkupSafe-3.0.2 altair-5.5.0 blinker-1.9.0 cachetools-5.5.1 click-8.1.8 gitdb-4.0.12 gitpython-3.1.44 jinja2-3.1.5 jsonschema-4.23.0 jsonschema-specifications-2024.10.1 markdown-it-py-3.0.0 mdurl-0.1.2 narwhals-1.25.2 pandas-2.2.3 pillow-11.1.0 protobuf-5.29.3 pyarrow-19.0.0 pydeck-0.9.1 pytz-2025.1 referencing-0.36.2 rich-13.9.4 rpds-py-0.22.3 smmap-5.0.2 streamlit-1.42.0 toml-0.10.2 tzdata-2025.1 watchdog-6.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T13:58:08.102672Z",
     "iopub.status.busy": "2025-02-08T13:58:08.102337Z",
     "iopub.status.idle": "2025-02-08T13:58:11.270749Z",
     "shell.execute_reply": "2025-02-08T13:58:11.269662Z",
     "shell.execute_reply.started": "2025-02-08T13:58:08.102645Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain_huggingface\n",
      "  Downloading langchain_huggingface-0.1.2-py3-none-any.whl (21 kB)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from langchain_huggingface) (0.3.34)\n",
      "Collecting transformers>=4.39.0\n",
      "  Downloading transformers-4.48.3-py3-none-any.whl (9.7 MB)\n",
      "     ---------------------------------------- 9.7/9.7 MB 10.1 MB/s eta 0:00:00\n",
      "Collecting sentence-transformers>=2.6.0\n",
      "  Downloading sentence_transformers-3.4.1-py3-none-any.whl (275 kB)\n",
      "     -------------------------------------- 275.9/275.9 kB 4.2 MB/s eta 0:00:00\n",
      "Collecting tokenizers>=0.19.1\n",
      "  Using cached tokenizers-0.21.0-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "Collecting huggingface-hub>=0.23.0\n",
      "  Downloading huggingface_hub-0.28.1-py3-none-any.whl (464 kB)\n",
      "     -------------------------------------- 464.1/464.1 kB 7.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (2.32.3)\n",
      "Collecting tqdm>=4.42.1\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (24.2)\n",
      "Collecting filelock\n",
      "  Using cached filelock-3.17.0-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (4.12.2)\n",
      "Collecting fsspec>=2023.5.0\n",
      "  Downloading fsspec-2025.2.0-py3-none-any.whl (184 kB)\n",
      "     ------------------------------------- 184.5/184.5 kB 10.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (9.0.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (1.33)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (0.3.7)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (2.10.6)\n",
      "Collecting torch>=1.11.0\n",
      "  Downloading torch-2.6.0-cp310-cp310-win_amd64.whl (204.2 MB)\n",
      "     -------------------------------------- 204.2/204.2 MB 5.2 MB/s eta 0:00:00\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.6.1-cp310-cp310-win_amd64.whl (11.1 MB)\n",
      "Collecting scipy\n",
      "  Using cached scipy-1.15.1-cp310-cp310-win_amd64.whl (43.9 MB)\n",
      "Requirement already satisfied: Pillow in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (11.1.0)\n",
      "Collecting safetensors>=0.4.1\n",
      "  Using cached safetensors-0.5.2-cp38-abi3-win_amd64.whl (303 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from transformers>=4.39.0->langchain_huggingface) (1.26.4)\n",
      "Collecting regex!=2019.12.17\n",
      "  Using cached regex-2024.11.6-cp310-cp310-win_amd64.whl (274 kB)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (0.28.1)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (0.23.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (3.10.15)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (2025.1.31)\n",
      "Collecting networkx\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "Collecting sympy==1.13.1\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (3.1.5)\n",
      "Collecting mpmath<1.4,>=1.1.0\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.23.0->langchain_huggingface) (0.4.6)\n",
      "Collecting threadpoolctl>=3.1.0\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Collecting joblib>=1.2.0\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (1.0.7)\n",
      "Requirement already satisfied: anyio in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (4.8.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (0.14.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (3.0.2)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (1.2.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (1.3.1)\n",
      "Installing collected packages: mpmath, tqdm, threadpoolctl, sympy, scipy, safetensors, regex, networkx, joblib, fsspec, filelock, torch, scikit-learn, huggingface-hub, tokenizers, transformers, sentence-transformers, langchain_huggingface\n",
      "Successfully installed filelock-3.17.0 fsspec-2025.2.0 huggingface-hub-0.28.1 joblib-1.4.2 langchain_huggingface-0.1.2 mpmath-1.3.0 networkx-3.4.2 regex-2024.11.6 safetensors-0.5.2 scikit-learn-1.6.1 scipy-1.15.1 sentence-transformers-3.4.1 sympy-1.13.1 threadpoolctl-3.5.0 tokenizers-0.21.0 torch-2.6.0 tqdm-4.67.1 transformers-4.48.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain_huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T13:58:31.051100Z",
     "iopub.status.busy": "2025-02-08T13:58:31.050846Z",
     "iopub.status.idle": "2025-02-08T13:58:35.007640Z",
     "shell.execute_reply": "2025-02-08T13:58:35.006748Z",
     "shell.execute_reply.started": "2025-02-08T13:58:31.051078Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pypdf\n",
      "  Downloading pypdf-5.2.0-py3-none-any.whl (298 kB)\n",
      "     -------------------------------------- 298.7/298.7 kB 1.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing_extensions>=4.0 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from pypdf) (4.12.2)\n",
      "Installing collected packages: pypdf\n",
      "Successfully installed pypdf-5.2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "#!pip install langchain openai\n",
    "!pip install pypdf\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "#from langchain.embeddings import OpenAIEmbeddings\n",
    "#from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T13:58:41.720180Z",
     "iopub.status.busy": "2025-02-08T13:58:41.719889Z",
     "iopub.status.idle": "2025-02-08T13:58:45.075252Z",
     "shell.execute_reply": "2025-02-08T13:58:45.074385Z",
     "shell.execute_reply.started": "2025-02-08T13:58:41.720157Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyngrok\n",
      "  Downloading pyngrok-7.2.3-py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: PyYAML>=5.1 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from pyngrok) (6.0.2)\n",
      "Installing collected packages: pyngrok\n",
      "Successfully installed pyngrok-7.2.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install pyngrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T14:06:48.317433Z",
     "iopub.status.busy": "2025-02-08T14:06:48.317071Z",
     "iopub.status.idle": "2025-02-08T14:06:51.764964Z",
     "shell.execute_reply": "2025-02-08T14:06:51.764064Z",
     "shell.execute_reply.started": "2025-02-08T14:06:48.317397Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting huggingface_hub==0.27.1\n",
      "  Using cached huggingface_hub-0.27.1-py3-none-any.whl (450 kB)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from huggingface_hub==0.27.1) (4.67.1)\n",
      "Requirement already satisfied: requests in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from huggingface_hub==0.27.1) (2.32.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from huggingface_hub==0.27.1) (6.0.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from huggingface_hub==0.27.1) (3.17.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from huggingface_hub==0.27.1) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from huggingface_hub==0.27.1) (4.12.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from huggingface_hub==0.27.1) (2025.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub==0.27.1) (0.4.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from requests->huggingface_hub==0.27.1) (2025.1.31)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from requests->huggingface_hub==0.27.1) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from requests->huggingface_hub==0.27.1) (2.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from requests->huggingface_hub==0.27.1) (3.10)\n",
      "Installing collected packages: huggingface_hub\n",
      "  Attempting uninstall: huggingface_hub\n",
      "    Found existing installation: huggingface-hub 0.28.1\n",
      "    Uninstalling huggingface-hub-0.28.1:\n",
      "      Successfully uninstalled huggingface-hub-0.28.1\n",
      "Successfully installed huggingface_hub-0.27.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install huggingface_hub==0.27.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T14:06:52.032891Z",
     "iopub.status.busy": "2025-02-08T14:06:52.032650Z",
     "iopub.status.idle": "2025-02-08T14:06:55.467254Z",
     "shell.execute_reply": "2025-02-08T14:06:55.466130Z",
     "shell.execute_reply.started": "2025-02-08T14:06:52.032869Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.48.0\n",
      "  Downloading transformers-4.48.0-py3-none-any.whl (9.7 MB)\n",
      "     ---------------------------------------- 9.7/9.7 MB 9.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from transformers==4.48.0) (6.0.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from transformers==4.48.0) (0.5.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from transformers==4.48.0) (1.26.4)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from transformers==4.48.0) (4.67.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from transformers==4.48.0) (3.17.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from transformers==4.48.0) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from transformers==4.48.0) (0.21.0)\n",
      "Requirement already satisfied: requests in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from transformers==4.48.0) (2.32.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from transformers==4.48.0) (24.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from transformers==4.48.0) (0.27.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.48.0) (2025.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.48.0) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from tqdm>=4.27->transformers==4.48.0) (0.4.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from requests->transformers==4.48.0) (2.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from requests->transformers==4.48.0) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from requests->transformers==4.48.0) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from requests->transformers==4.48.0) (2025.1.31)\n",
      "Installing collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.48.3\n",
      "    Uninstalling transformers-4.48.3:\n",
      "      Successfully uninstalled transformers-4.48.3\n",
      "Successfully installed transformers-4.48.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.48.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T14:06:55.468898Z",
     "iopub.status.busy": "2025-02-08T14:06:55.468641Z",
     "iopub.status.idle": "2025-02-08T14:06:59.128637Z",
     "shell.execute_reply": "2025-02-08T14:06:59.127617Z",
     "shell.execute_reply.started": "2025-02-08T14:06:55.468877Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama_index\n",
      "  Downloading llama_index-0.12.16-py3-none-any.whl (6.9 kB)\n",
      "Collecting llama-index-llms-openai<0.4.0,>=0.3.0\n",
      "  Downloading llama_index_llms_openai-0.3.18-py3-none-any.whl (14 kB)\n",
      "Collecting llama-index-program-openai<0.4.0,>=0.3.0\n",
      "  Using cached llama_index_program_openai-0.3.1-py3-none-any.whl (5.3 kB)\n",
      "Collecting nltk>3.8.1\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Collecting llama-index-readers-file<0.5.0,>=0.4.0\n",
      "  Using cached llama_index_readers_file-0.4.4-py3-none-any.whl (39 kB)\n",
      "Collecting llama-index-agent-openai<0.5.0,>=0.4.0\n",
      "  Downloading llama_index_agent_openai-0.4.3-py3-none-any.whl (13 kB)\n",
      "Collecting llama-index-embeddings-openai<0.4.0,>=0.3.0\n",
      "  Using cached llama_index_embeddings_openai-0.3.1-py3-none-any.whl (6.2 kB)\n",
      "Collecting llama-index-core<0.13.0,>=0.12.16\n",
      "  Downloading llama_index_core-0.12.16.post1-py3-none-any.whl (1.6 MB)\n",
      "     ---------------------------------------- 1.6/1.6 MB 4.2 MB/s eta 0:00:00\n",
      "Collecting llama-index-question-gen-openai<0.4.0,>=0.3.0\n",
      "  Using cached llama_index_question_gen_openai-0.3.0-py3-none-any.whl (2.9 kB)\n",
      "Collecting llama-index-multi-modal-llms-openai<0.5.0,>=0.4.0\n",
      "  Downloading llama_index_multi_modal_llms_openai-0.4.3-py3-none-any.whl (5.9 kB)\n",
      "Collecting llama-index-cli<0.5.0,>=0.4.0\n",
      "  Using cached llama_index_cli-0.4.0-py3-none-any.whl (27 kB)\n",
      "Collecting llama-index-indices-managed-llama-cloud>=0.4.0\n",
      "  Using cached llama_index_indices_managed_llama_cloud-0.6.4-py3-none-any.whl (13 kB)\n",
      "Collecting llama-index-readers-llama-parse>=0.4.0\n",
      "  Using cached llama_index_readers_llama_parse-0.4.0-py3-none-any.whl (2.5 kB)\n",
      "Collecting openai>=1.14.0\n",
      "  Downloading openai-1.61.1-py3-none-any.whl (463 kB)\n",
      "     -------------------------------------- 463.1/463.1 kB 4.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.16->llama_index) (9.0.0)\n",
      "Requirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.16->llama_index) (2.0.38)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.16->llama_index) (3.4.2)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.16->llama_index) (4.12.2)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.16->llama_index) (0.9.0)\n",
      "Requirement already satisfied: dataclasses-json in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.16->llama_index) (0.6.7)\n",
      "Requirement already satisfied: numpy in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.16->llama_index) (1.26.4)\n",
      "Requirement already satisfied: pydantic>=2.8.0 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.16->llama_index) (2.10.6)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.16->llama_index) (3.11.12)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.16->llama_index) (4.67.1)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.16->llama_index) (1.6.0)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.16->llama_index) (6.0.2)\n",
      "Collecting filetype<2.0.0,>=1.2.0\n",
      "  Using cached filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Collecting wrapt\n",
      "  Using cached wrapt-1.17.2-cp310-cp310-win_amd64.whl (38 kB)\n",
      "Collecting tiktoken>=0.3.3\n",
      "  Using cached tiktoken-0.8.0-cp310-cp310-win_amd64.whl (884 kB)\n",
      "Collecting deprecated>=1.2.9.3\n",
      "  Downloading Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n",
      "Requirement already satisfied: requests>=2.31.0 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.16->llama_index) (2.32.3)\n",
      "Requirement already satisfied: httpx in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.16->llama_index) (0.28.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.16->llama_index) (2025.2.0)\n",
      "Collecting dirtyjson<2.0.0,>=1.0.8\n",
      "  Using cached dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: pillow>=9.0.0 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.16->llama_index) (11.1.0)\n",
      "Collecting llama-cloud<0.2.0,>=0.1.8\n",
      "  Downloading llama_cloud-0.1.11-py3-none-any.whl (250 kB)\n",
      "     -------------------------------------- 250.6/250.6 kB 2.2 MB/s eta 0:00:00\n",
      "Collecting striprtf<0.0.27,>=0.0.26\n",
      "  Using cached striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
      "Collecting beautifulsoup4<5.0.0,>=4.12.3\n",
      "  Downloading beautifulsoup4-4.13.3-py3-none-any.whl (186 kB)\n",
      "     -------------------------------------- 186.0/186.0 kB 2.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pandas in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama_index) (2.2.3)\n",
      "Requirement already satisfied: pypdf<6.0.0,>=5.1.0 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama_index) (5.2.0)\n",
      "Collecting llama-parse>=0.5.0\n",
      "  Downloading llama_parse-0.6.0-py3-none-any.whl (4.8 kB)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from nltk>3.8.1->llama_index) (2024.11.6)\n",
      "Requirement already satisfied: joblib in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from nltk>3.8.1->llama_index) (1.4.2)\n",
      "Requirement already satisfied: click in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from nltk>3.8.1->llama_index) (8.1.8)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.16->llama_index) (1.5.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.16->llama_index) (4.0.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.16->llama_index) (1.3.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.16->llama_index) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.16->llama_index) (1.18.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.16->llama_index) (25.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.16->llama_index) (6.1.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.16->llama_index) (2.4.6)\n",
      "Collecting soupsieve>1.2\n",
      "  Using cached soupsieve-2.6-py3-none-any.whl (36 kB)\n",
      "Collecting certifi<2025.0.0,>=2024.7.4\n",
      "  Using cached certifi-2024.12.14-py3-none-any.whl (164 kB)\n",
      "Requirement already satisfied: anyio in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from httpx->llama-index-core<0.13.0,>=0.12.16->llama_index) (4.8.0)\n",
      "Requirement already satisfied: idna in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from httpx->llama-index-core<0.13.0,>=0.12.16->llama_index) (3.10)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from httpx->llama-index-core<0.13.0,>=0.12.16->llama_index) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.16->llama_index) (0.14.0)\n",
      "Collecting llama-cloud-services\n",
      "  Downloading llama_cloud_services-0.6.0-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: sniffio in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama_index) (1.3.1)\n",
      "Collecting distro<2,>=1.7.0\n",
      "  Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Collecting jiter<1,>=0.4.0\n",
      "  Using cached jiter-0.8.2-cp310-cp310-win_amd64.whl (204 kB)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.16->llama_index) (2.27.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.16->llama_index) (0.7.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.16->llama_index) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.16->llama_index) (2.3.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.16->llama_index) (3.1.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from tqdm<5.0.0,>=4.66.1->llama-index-core<0.13.0,>=0.12.16->llama_index) (0.4.6)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.16->llama_index) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.16->llama_index) (3.26.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama_index) (2025.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama_index) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama_index) (2025.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from anyio->httpx->llama-index-core<0.13.0,>=0.12.16->llama_index) (1.2.2)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13.0,>=0.12.16->llama_index) (24.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama_index) (1.17.0)\n",
      "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.1 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from llama-cloud-services->llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama_index) (1.0.1)\n",
      "Installing collected packages: striprtf, filetype, dirtyjson, wrapt, soupsieve, jiter, distro, certifi, nltk, deprecated, beautifulsoup4, tiktoken, openai, llama-index-core, llama-cloud, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-cloud-services, llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, llama-index-readers-llama-parse, llama-index-program-openai, llama-index-question-gen-openai, llama_index\n",
      "  Attempting uninstall: certifi\n",
      "    Found existing installation: certifi 2025.1.31\n",
      "    Uninstalling certifi-2025.1.31:\n",
      "      Successfully uninstalled certifi-2025.1.31\n",
      "Successfully installed beautifulsoup4-4.13.3 certifi-2024.12.14 deprecated-1.2.18 dirtyjson-1.0.8 distro-1.9.0 filetype-1.2.0 jiter-0.8.2 llama-cloud-0.1.11 llama-cloud-services-0.6.0 llama-index-agent-openai-0.4.3 llama-index-cli-0.4.0 llama-index-core-0.12.16.post1 llama-index-embeddings-openai-0.3.1 llama-index-indices-managed-llama-cloud-0.6.4 llama-index-llms-openai-0.3.18 llama-index-multi-modal-llms-openai-0.4.3 llama-index-program-openai-0.3.1 llama-index-question-gen-openai-0.3.0 llama-index-readers-file-0.4.4 llama-index-readers-llama-parse-0.4.0 llama-parse-0.6.0 llama_index-0.12.16 nltk-3.9.1 openai-1.61.1 soupsieve-2.6 striprtf-0.0.26 tiktoken-0.8.0 wrapt-1.17.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install llama_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T14:06:59.130756Z",
     "iopub.status.busy": "2025-02-08T14:06:59.130452Z",
     "iopub.status.idle": "2025-02-08T14:07:02.910757Z",
     "shell.execute_reply": "2025-02-08T14:07:02.909676Z",
     "shell.execute_reply.started": "2025-02-08T14:06:59.130731Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama_index.embeddings.huggingface\n",
      "  Downloading llama_index_embeddings_huggingface-0.5.1-py3-none-any.whl (8.9 kB)\n",
      "Requirement already satisfied: sentence-transformers>=2.6.1 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from llama_index.embeddings.huggingface) (3.4.1)\n",
      "Requirement already satisfied: huggingface-hub[inference]>=0.19.0 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from llama_index.embeddings.huggingface) (0.27.1)\n",
      "Requirement already satisfied: llama-index-core<0.13.0,>=0.12.0 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from llama_index.embeddings.huggingface) (0.12.16.post1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from huggingface-hub[inference]>=0.19.0->llama_index.embeddings.huggingface) (4.12.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from huggingface-hub[inference]>=0.19.0->llama_index.embeddings.huggingface) (6.0.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from huggingface-hub[inference]>=0.19.0->llama_index.embeddings.huggingface) (3.17.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from huggingface-hub[inference]>=0.19.0->llama_index.embeddings.huggingface) (24.2)\n",
      "Requirement already satisfied: requests in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from huggingface-hub[inference]>=0.19.0->llama_index.embeddings.huggingface) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from huggingface-hub[inference]>=0.19.0->llama_index.embeddings.huggingface) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from huggingface-hub[inference]>=0.19.0->llama_index.embeddings.huggingface) (2025.2.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from huggingface-hub[inference]>=0.19.0->llama_index.embeddings.huggingface) (3.11.12)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama_index.embeddings.huggingface) (1.0.8)\n",
      "Requirement already satisfied: numpy in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama_index.embeddings.huggingface) (1.26.4)\n",
      "Requirement already satisfied: pillow>=9.0.0 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama_index.embeddings.huggingface) (11.1.0)\n",
      "Requirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama_index.embeddings.huggingface) (2.0.38)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama_index.embeddings.huggingface) (1.2.18)\n",
      "Requirement already satisfied: nltk>3.8.1 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama_index.embeddings.huggingface) (3.9.1)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama_index.embeddings.huggingface) (3.4.2)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama_index.embeddings.huggingface) (1.2.0)\n",
      "Requirement already satisfied: httpx in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama_index.embeddings.huggingface) (0.28.1)\n",
      "Requirement already satisfied: wrapt in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama_index.embeddings.huggingface) (1.17.2)\n",
      "Requirement already satisfied: pydantic>=2.8.0 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama_index.embeddings.huggingface) (2.10.6)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama_index.embeddings.huggingface) (9.0.0)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama_index.embeddings.huggingface) (1.6.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama_index.embeddings.huggingface) (0.9.0)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama_index.embeddings.huggingface) (0.8.0)\n",
      "Requirement already satisfied: dataclasses-json in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama_index.embeddings.huggingface) (0.6.7)\n",
      "Requirement already satisfied: scipy in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from sentence-transformers>=2.6.1->llama_index.embeddings.huggingface) (1.15.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from sentence-transformers>=2.6.1->llama_index.embeddings.huggingface) (4.48.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from sentence-transformers>=2.6.1->llama_index.embeddings.huggingface) (1.6.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from sentence-transformers>=2.6.1->llama_index.embeddings.huggingface) (2.6.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama_index.embeddings.huggingface) (4.0.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama_index.embeddings.huggingface) (6.1.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama_index.embeddings.huggingface) (2.4.6)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama_index.embeddings.huggingface) (1.5.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama_index.embeddings.huggingface) (0.2.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama_index.embeddings.huggingface) (25.1.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama_index.embeddings.huggingface) (1.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama_index.embeddings.huggingface) (1.18.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama_index.embeddings.huggingface) (2024.11.6)\n",
      "Requirement already satisfied: click in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama_index.embeddings.huggingface) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama_index.embeddings.huggingface) (1.4.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama_index.embeddings.huggingface) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama_index.embeddings.huggingface) (2.27.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from requests->huggingface-hub[inference]>=0.19.0->llama_index.embeddings.huggingface) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from requests->huggingface-hub[inference]>=0.19.0->llama_index.embeddings.huggingface) (2024.12.14)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from requests->huggingface-hub[inference]>=0.19.0->llama_index.embeddings.huggingface) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from requests->huggingface-hub[inference]>=0.19.0->llama_index.embeddings.huggingface) (3.10)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama_index.embeddings.huggingface) (3.1.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama_index.embeddings.huggingface) (3.1.5)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama_index.embeddings.huggingface) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=2.6.1->llama_index.embeddings.huggingface) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub[inference]>=0.19.0->llama_index.embeddings.huggingface) (0.4.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.6.1->llama_index.embeddings.huggingface) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.6.1->llama_index.embeddings.huggingface) (0.5.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.0->llama_index.embeddings.huggingface) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama_index.embeddings.huggingface) (3.26.1)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama_index.embeddings.huggingface) (1.0.7)\n",
      "Requirement already satisfied: anyio in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama_index.embeddings.huggingface) (4.8.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.0->llama_index.embeddings.huggingface) (0.14.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from scikit-learn->sentence-transformers>=2.6.1->llama_index.embeddings.huggingface) (3.5.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from anyio->httpx->llama-index-core<0.13.0,>=0.12.0->llama_index.embeddings.huggingface) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from anyio->httpx->llama-index-core<0.13.0,>=0.12.0->llama_index.embeddings.huggingface) (1.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\usar\\documents\\work\\aai\\.venv\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.1->llama_index.embeddings.huggingface) (3.0.2)\n",
      "Installing collected packages: llama_index.embeddings.huggingface\n",
      "Successfully installed llama_index.embeddings.huggingface-0.5.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install llama_index.embeddings.huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T13:59:12.036774Z",
     "iopub.status.busy": "2025-02-08T13:59:12.036447Z",
     "iopub.status.idle": "2025-02-08T13:59:12.153017Z",
     "shell.execute_reply": "2025-02-08T13:59:12.152437Z",
     "shell.execute_reply.started": "2025-02-08T13:59:12.036746Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document  #represents piece of text associated with metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T13:59:12.155245Z",
     "iopub.status.busy": "2025-02-08T13:59:12.155032Z",
     "iopub.status.idle": "2025-02-08T13:59:12.571711Z",
     "shell.execute_reply": "2025-02-08T13:59:12.571067Z",
     "shell.execute_reply.started": "2025-02-08T13:59:12.155227Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "#import fitz  # PyMuPDF for PDF text extraction\n",
    "#import faiss\n",
    "import numpy as np\n",
    "#from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T13:59:12.573438Z",
     "iopub.status.busy": "2025-02-08T13:59:12.572780Z",
     "iopub.status.idle": "2025-02-08T13:59:12.922971Z",
     "shell.execute_reply": "2025-02-08T13:59:12.922267Z",
     "shell.execute_reply.started": "2025-02-08T13:59:12.573407Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.27.1\n",
      "4.48.0\n"
     ]
    }
   ],
   "source": [
    "import huggingface_hub, transformers\n",
    "print(huggingface_hub.__version__)\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T13:59:12.924402Z",
     "iopub.status.busy": "2025-02-08T13:59:12.923867Z",
     "iopub.status.idle": "2025-02-08T13:59:12.927944Z",
     "shell.execute_reply": "2025-02-08T13:59:12.927152Z",
     "shell.execute_reply.started": "2025-02-08T13:59:12.924355Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import file_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T13:59:12.928992Z",
     "iopub.status.busy": "2025-02-08T13:59:12.928701Z",
     "iopub.status.idle": "2025-02-08T13:59:12.942976Z",
     "shell.execute_reply": "2025-02-08T13:59:12.942323Z",
     "shell.execute_reply.started": "2025-02-08T13:59:12.928961Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_doc(data_path):\n",
    "  doc_loader = PyPDFLoader(data_path)\n",
    "  doc = doc_loader.load()\n",
    "  return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T13:59:12.944094Z",
     "iopub.status.busy": "2025-02-08T13:59:12.943822Z",
     "iopub.status.idle": "2025-02-08T13:59:12.957612Z",
     "shell.execute_reply": "2025-02-08T13:59:12.956949Z",
     "shell.execute_reply.started": "2025-02-08T13:59:12.944065Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def split_documents(documents: list[Document]):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=800,\n",
    "        chunk_overlap=80,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "    return text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T14:00:07.076843Z",
     "iopub.status.busy": "2025-02-08T14:00:07.076557Z",
     "iopub.status.idle": "2025-02-08T14:00:07.141342Z",
     "shell.execute_reply": "2025-02-08T14:00:07.140681Z",
     "shell.execute_reply.started": "2025-02-08T14:00:07.076822Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Login with your Hugging Face token\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T14:00:09.827796Z",
     "iopub.status.busy": "2025-02-08T14:00:09.827511Z",
     "iopub.status.idle": "2025-02-08T14:03:01.556272Z",
     "shell.execute_reply": "2025-02-08T14:03:01.555363Z",
     "shell.execute_reply.started": "2025-02-08T14:00:09.827773Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a6a6ed31261458db3d0f33b958739f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df893513955c46cd9949a94866f5052f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5920a9ca6e434089814833ce2ebedc4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "835a93e52ceb410db865adc79d485d3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/878 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01173dceb2684a25af14223813062903",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f75d98ea2f2491cb25c62f324ef88a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f72885ad2fc94219899055ec3b59d399",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7196901c0356436fa50beafb8fb4d49e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e308998eaa104e169d1faebba28edf86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07969473f6fe4a52b780d3a7e849ae9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model directly\n",
    "#import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T12:58:27.150557Z",
     "iopub.status.busy": "2025-02-08T12:58:27.150015Z",
     "iopub.status.idle": "2025-02-08T12:58:27.287098Z",
     "shell.execute_reply": "2025-02-08T12:58:27.286411Z",
     "shell.execute_reply.started": "2025-02-08T12:58:27.150532Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import file_download\n",
    "from transformers import pipeline, AutoConfig\n",
    "config_file = file_download.hf_hub_download(\n",
    "    repo_id=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "    filename=\"config.json\"\n",
    "    #use_auth_token=\"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\"  # Explicitly provide the token\n",
    ")\n",
    "config = AutoConfig.from_pretrained(config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T12:58:27.288157Z",
     "iopub.status.busy": "2025-02-08T12:58:27.287925Z",
     "iopub.status.idle": "2025-02-08T12:58:31.627252Z",
     "shell.execute_reply": "2025-02-08T12:58:31.626592Z",
     "shell.execute_reply.started": "2025-02-08T12:58:27.288137Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"text-generation\",\n",
    "                model=model,\n",
    "                tokenizer = tokenizer,\n",
    "                config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-01-22T07:31:04.492791Z",
     "iopub.status.busy": "2025-01-22T07:31:04.492456Z",
     "iopub.status.idle": "2025-01-22T07:31:04.509642Z",
     "shell.execute_reply": "2025-01-22T07:31:04.508467Z",
     "shell.execute_reply.started": "2025-01-22T07:31:04.49276Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# prompt = \"complete the following sentence. The rainbow is...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-01-22T07:31:04.511392Z",
     "iopub.status.busy": "2025-01-22T07:31:04.510912Z",
     "iopub.status.idle": "2025-01-22T07:31:04.528093Z",
     "shell.execute_reply": "2025-01-22T07:31:04.526999Z",
     "shell.execute_reply.started": "2025-01-22T07:31:04.511346Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# extracted_text = pipe(prompt,max_new_tokens=500, num_return_sequences=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-01-22T07:31:04.529538Z",
     "iopub.status.busy": "2025-01-22T07:31:04.52917Z",
     "iopub.status.idle": "2025-01-22T07:31:04.54705Z",
     "shell.execute_reply": "2025-01-22T07:31:04.545968Z",
     "shell.execute_reply.started": "2025-01-22T07:31:04.529507Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# generated_text = extracted_text[0]['generated_text'].replace(prompt, \"\").strip()\n",
    "\n",
    "# # Print or use the generated text\n",
    "# print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": false,
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-01-22T07:31:04.548869Z",
     "iopub.status.busy": "2025-01-22T07:31:04.548456Z",
     "iopub.status.idle": "2025-01-22T07:31:12.999646Z",
     "shell.execute_reply": "2025-01-22T07:31:12.998063Z",
     "shell.execute_reply.started": "2025-01-22T07:31:04.548821Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !pip install llama_index.embeddings.huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-01-22T07:31:13.001342Z",
     "iopub.status.busy": "2025-01-22T07:31:13.001027Z",
     "iopub.status.idle": "2025-01-22T07:31:13.006317Z",
     "shell.execute_reply": "2025-01-22T07:31:13.005199Z",
     "shell.execute_reply.started": "2025-01-22T07:31:13.001315Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#!pip install --upgrade transformers sentence-transformers huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-01-22T07:31:13.008108Z",
     "iopub.status.busy": "2025-01-22T07:31:13.007728Z",
     "iopub.status.idle": "2025-01-22T07:31:13.027742Z",
     "shell.execute_reply": "2025-01-22T07:31:13.026747Z",
     "shell.execute_reply.started": "2025-01-22T07:31:13.008077Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import huggingface_hub, transformers\n",
    "# print(huggingface_hub.__version__)\n",
    "# print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-01-22T07:31:13.035864Z",
     "iopub.status.busy": "2025-01-22T07:31:13.03549Z",
     "iopub.status.idle": "2025-01-22T07:31:21.618951Z",
     "shell.execute_reply": "2025-01-22T07:31:21.617512Z",
     "shell.execute_reply.started": "2025-01-22T07:31:13.035834Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !pip install llama_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-01-22T07:31:21.622895Z",
     "iopub.status.busy": "2025-01-22T07:31:21.622477Z",
     "iopub.status.idle": "2025-01-22T07:31:26.557307Z",
     "shell.execute_reply": "2025-01-22T07:31:26.555791Z",
     "shell.execute_reply.started": "2025-01-22T07:31:21.622843Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !pip install llama_index.embeddings.huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-01-22T07:31:26.559248Z",
     "iopub.status.busy": "2025-01-22T07:31:26.558772Z",
     "iopub.status.idle": "2025-01-22T07:31:26.564545Z",
     "shell.execute_reply": "2025-01-22T07:31:26.563188Z",
     "shell.execute_reply.started": "2025-01-22T07:31:26.559214Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#!pip install --upgrade sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T13:12:45.110426Z",
     "iopub.status.busy": "2025-02-08T13:12:45.109955Z",
     "iopub.status.idle": "2025-02-08T13:12:57.278283Z",
     "shell.execute_reply": "2025-02-08T13:12:57.277504Z",
     "shell.execute_reply.started": "2025-02-08T13:12:45.110384Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ead499ed16844610ae2037d5037a1997",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3e44faa01344478ac0d8a74d95f0b8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7e560853cf94213a24d1d48a4e0fb5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/94.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8c95ddcd01e4664bb116ec535c4fb6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61925ca7c40848e6b1bd67d5d7cdb532",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/779 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed4c0afd7e874e688f4152f66c44c14f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d14aff46745644608306b6c58ed6dd0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e568ecaca98d4c71a4d838c3f5ac2063",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfc727aa76cb410aad85adab83c43156",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1856f5e18ac4d689c2b55688d71186a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9ac7adeeac94d8fb1e236b1207ec1e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/191 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model directly\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-large-en-v1.5\", trust_remote_code=True)\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"BAAI/bge-large-en-v1.5\")\n",
    "#model = AutoModel.from_pretrained(\"BAAI/bge-large-en-v1.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-01-22T07:31:46.389517Z",
     "iopub.status.busy": "2025-01-22T07:31:46.389067Z",
     "iopub.status.idle": "2025-01-22T07:32:59.945928Z",
     "shell.execute_reply": "2025-01-22T07:32:59.944697Z",
     "shell.execute_reply.started": "2025-01-22T07:31:46.389472Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# pip install --upgrade --force-reinstall llama-index-readers-file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T13:18:44.551673Z",
     "iopub.status.busy": "2025-02-08T13:18:44.551325Z",
     "iopub.status.idle": "2025-02-08T13:18:44.603151Z",
     "shell.execute_reply": "2025-02-08T13:18:44.602293Z",
     "shell.execute_reply.started": "2025-02-08T13:18:44.551648Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#from llama_index import DocxReader\n",
    "#from llama_index.readers.file import DocxReader\n",
    "#import llama_index.readers.file\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "input_dir_path = \"/kaggle/input/sample/\"\n",
    "# load data\n",
    "loader = SimpleDirectoryReader(\n",
    "            input_dir = input_dir_path,\n",
    "            required_exts=[\".pdf\"],\n",
    "            recursive=True\n",
    "        )\n",
    "docs = loader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T13:12:15.470731Z",
     "iopub.status.busy": "2025-02-08T13:12:15.469886Z",
     "iopub.status.idle": "2025-02-08T13:12:15.475560Z",
     "shell.execute_reply": "2025-02-08T13:12:15.474518Z",
     "shell.execute_reply.started": "2025-02-08T13:12:15.470698Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T13:12:57.279723Z",
     "iopub.status.busy": "2025-02-08T13:12:57.279444Z",
     "iopub.status.idle": "2025-02-08T13:12:58.492648Z",
     "shell.execute_reply": "2025-02-08T13:12:58.491632Z",
     "shell.execute_reply.started": "2025-02-08T13:12:57.279700Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4b3dd7d93b4410494020ce1c5d1da04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "# ====== Create vector store and upload indexed data ======\n",
    "Settings.embed_model = embed_model # we specify the embedding model to be used\n",
    "index = VectorStoreIndex.from_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T13:18:49.752084Z",
     "iopub.status.busy": "2025-02-08T13:18:49.751804Z",
     "iopub.status.idle": "2025-02-08T13:18:49.833743Z",
     "shell.execute_reply": "2025-02-08T13:18:49.833038Z",
     "shell.execute_reply.started": "2025-02-08T13:18:49.752063Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97da1609377a4bc5a4bbe71dfa1fdc8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4259a7ef6ee4ecf909cfc0d38daf529",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "user_query = \"What is this about ?\"  # Example query\n",
    "query_embedding = embed_model.get_text_embedding(user_query)  # Generate query embedding\n",
    "\n",
    "retriever = index.as_retriever()\n",
    "results = retriever.retrieve(user_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T14:03:01.558424Z",
     "iopub.status.busy": "2025-02-08T14:03:01.557711Z",
     "iopub.status.idle": "2025-02-08T14:03:01.782674Z",
     "shell.execute_reply": "2025-02-08T14:03:01.781119Z",
     "shell.execute_reply.started": "2025-02-08T14:03:01.558379Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-30f40d2037e2>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mretrieved_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Based on the following context, answer the question:\\n\\nContext:\\n{retrieved_texts}\\n\\nQuestion: {user_query}\\nAnswer:\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4096\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Generate the response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "retrieved_texts = \"\\n\".join([doc.text for doc in results])\n",
    "prompt = f\"Based on the following context, answer the question:\\n\\nContext:\\n{retrieved_texts}\\n\\nQuestion: {user_query}\\nAnswer:\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=4096)\n",
    "\n",
    "# Generate the response\n",
    "output = model.generate(**inputs, max_new_tokens=300, do_sample=True)\n",
    "\n",
    "# Decode and print the response\n",
    "#response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "response = tokenizer.decode(output[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-01-22T07:35:40.307416Z",
     "iopub.status.busy": "2025-01-22T07:35:40.307044Z",
     "iopub.status.idle": "2025-01-22T07:35:40.312847Z",
     "shell.execute_reply": "2025-01-22T07:35:40.31136Z",
     "shell.execute_reply.started": "2025-01-22T07:35:40.307385Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !pip install cloudflared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-01-22T07:35:40.315139Z",
     "iopub.status.busy": "2025-01-22T07:35:40.314632Z",
     "iopub.status.idle": "2025-01-22T07:35:45.45527Z",
     "shell.execute_reply": "2025-01-22T07:35:45.453847Z",
     "shell.execute_reply.started": "2025-01-22T07:35:40.31509Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !pip install pyngrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T13:21:22.409687Z",
     "iopub.status.busy": "2025-02-08T13:21:22.409373Z",
     "iopub.status.idle": "2025-02-08T13:21:22.416035Z",
     "shell.execute_reply": "2025-02-08T13:21:22.415113Z",
     "shell.execute_reply.started": "2025-02-08T13:21:22.409665Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_response_from_docs(input_dir_path, user_query, embed_model):\n",
    "    # Load PDFs\n",
    "    loader = SimpleDirectoryReader(input_dir=input_dir_path, required_exts=[\".pdf\"], recursive=True)\n",
    "    docs = loader.load_data()\n",
    "\n",
    "    # Create vector index and retriever\n",
    "    Settings.embed_model = embed_model\n",
    "    index = VectorStoreIndex.from_documents(docs)\n",
    "    retriever = index.as_retriever()\n",
    "    \n",
    "    # Retrieve relevant documents\n",
    "    results = retriever.retrieve(user_query)\n",
    "    retrieved_texts = \"\\n\".join([doc.text for doc in results])\n",
    "\n",
    "    # Construct the prompt for LLM\n",
    "    prompt = f\"Based on the following context, answer the question:\\n\\nContext:\\n{retrieved_texts}\\n\\nQuestion: {user_query}\\nAnswer:\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=4096)\n",
    "\n",
    "    # Generate response\n",
    "    output = model.generate(**inputs, max_new_tokens=300, do_sample=True)\n",
    "\n",
    "    # Extract only the generated output (removing input prompt)\n",
    "    response = tokenizer.decode(output[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-01-22T07:35:46.003053Z",
     "iopub.status.busy": "2025-01-22T07:35:46.002644Z",
     "iopub.status.idle": "2025-01-22T07:35:47.597781Z",
     "shell.execute_reply": "2025-01-22T07:35:47.596469Z",
     "shell.execute_reply.started": "2025-01-22T07:35:46.003018Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !pip uninstall numpy -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-01-22T07:35:47.600385Z",
     "iopub.status.busy": "2025-01-22T07:35:47.600046Z",
     "iopub.status.idle": "2025-01-22T07:35:55.988061Z",
     "shell.execute_reply": "2025-01-22T07:35:55.986777Z",
     "shell.execute_reply.started": "2025-01-22T07:35:47.600357Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !pip install --upgrade numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "execution_failed": "2025-01-23T14:13:39.66Z",
     "iopub.execute_input": "2025-01-22T09:15:29.499604Z",
     "iopub.status.busy": "2025-01-22T09:15:29.499203Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !pip uninstall numpy\n",
    "# !pip install --no-cache-dir numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "execution_failed": "2025-01-23T14:13:39.661Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T14:05:51.977224Z",
     "iopub.status.busy": "2025-02-08T14:05:51.976859Z",
     "iopub.status.idle": "2025-02-08T14:05:55.633109Z",
     "shell.execute_reply": "2025-02-08T14:05:55.632249Z",
     "shell.execute_reply.started": "2025-02-08T14:05:51.977188Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: GPUtil in /usr/local/lib/python3.10/dist-packages (1.4.0)\n",
      "Initial GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% |  1% |\n",
      "|  1 |  0% |  0% |\n",
      "GPU Usage after emptying the cache\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  4% |  1% |\n",
      "|  1 |  0% |  0% |\n"
     ]
    }
   ],
   "source": [
    "!pip install GPUtil\n",
    "\n",
    "import torch\n",
    "from GPUtil import showUtilization as gpu_usage\n",
    "from numba import cuda\n",
    "\n",
    "def free_gpu_cache():\n",
    "    print(\"Initial GPU Usage\")\n",
    "    gpu_usage()                             \n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    cuda.select_device(0)\n",
    "    cuda.close()\n",
    "    cuda.select_device(0)\n",
    "\n",
    "    print(\"GPU Usage after emptying the cache\")\n",
    "    gpu_usage()\n",
    "\n",
    "free_gpu_cache()                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T14:06:02.511080Z",
     "iopub.status.busy": "2025-02-08T14:06:02.510775Z",
     "iopub.status.idle": "2025-02-08T14:06:02.517176Z",
     "shell.execute_reply": "2025-02-08T14:06:02.516375Z",
     "shell.execute_reply.started": "2025-02-08T14:06:02.511055Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py\n",
    "\n",
    "import streamlit as st\n",
    "import os\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-large-en-v1.5\", trust_remote_code=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\")\n",
    "\n",
    "def generate_response_from_docs(retrieved_texts,user_query, embed_model):\n",
    "    \n",
    "\n",
    "    # Construct the prompt for LLM\n",
    "    prompt = f\"Based on the following context, answer the question:\\n\\nContext:\\n{retrieved_texts}\\n\\nQuestion: {user_query}\\nAnswer:\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=4096)\n",
    "\n",
    "    # Generate response\n",
    "    output = model.generate(**inputs, max_new_tokens=300, do_sample=True)\n",
    "\n",
    "    # Extract only the generated output (removing input prompt)\n",
    "    response = tokenizer.decode(output[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "    return response\n",
    "\n",
    "st.set_page_config(page_title=\"RAG Application\", layout=\"wide\")\n",
    "st.title(\" RAG-based LLM Query System\")\n",
    "\n",
    "# File uploader\n",
    "uploaded_file = st.file_uploader(\"Drop your PDF here\", type=[\"pdf\"])\n",
    "\n",
    "# Query input\n",
    "user_query = st.text_input(\"Ask a question about the document:\")\n",
    "\n",
    "if uploaded_file:\n",
    "    # Save uploaded PDF to a temporary directory\n",
    "    #input_dir_path = \"uploaded_pdfs\"\n",
    "    input_dir_path = \"/kaggle/working/uploaded_pdfs\"\n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(input_dir_path, exist_ok=True)\n",
    "    with open(f\"{input_dir_path}/document.pdf\", \"wb\") as f:\n",
    "        f.write(uploaded_file.getbuffer())\n",
    "    # Load PDFs\n",
    "    loader = SimpleDirectoryReader(input_dir_path, required_exts=[\".pdf\"], recursive=True)\n",
    "    docs = loader.load_data()\n",
    "\n",
    "    # Create vector index and retriever\n",
    "    Settings.embed_model = embed_model\n",
    "    index = VectorStoreIndex.from_documents(docs)\n",
    "    retriever = index.as_retriever()\n",
    "\n",
    "    # Load embedding model (replace with actual embedding model)\n",
    "    #embed_model = \"sentence-transformers/all-MiniLM-L6-v2\"  # Example\n",
    "\n",
    "    if user_query:\n",
    "\n",
    "        # Retrieve relevant documents\n",
    "        results = retriever.retrieve(user_query)\n",
    "        retrieved_texts = \"\\n\".join([doc.text for doc in results])\n",
    "    \n",
    "        # Generate response\n",
    "        response = generate_response_from_docs(retrieved_texts,user_query, embed_model)\n",
    "\n",
    "        # Display response\n",
    "        st.subheader(\"LLM Response:\")\n",
    "        st.write(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-02-08T14:09:50.677Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from pyngrok import ngrok\n",
    "import os\n",
    "ngrok.set_auth_token(\"2rkIdE1LOpOejwZ4FyiIXsxi53l_6h1vv4UWzPN2GwVUHu3ij\")\n",
    "\n",
    "# Start the streamlit app\n",
    "os.system(\"streamlit run app.py &\")\n",
    "\n",
    "# Set up a tunnel to the streamlit app (default port 8501)\n",
    "public_url = ngrok.connect(8501)\n",
    "\n",
    "print(f\"Streamlit is available at: {public_url}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-01-22T07:35:58.529482Z",
     "iopub.status.busy": "2025-01-22T07:35:58.529029Z",
     "iopub.status.idle": "2025-01-22T07:35:58.533784Z",
     "shell.execute_reply": "2025-01-22T07:35:58.532637Z",
     "shell.execute_reply.started": "2025-01-22T07:35:58.529446Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !streamlit run app.py --server.port 8501 & cloudflared tunnel run --token <TOKEN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-01-22T07:35:58.53532Z",
     "iopub.status.busy": "2025-01-22T07:35:58.534958Z",
     "iopub.status.idle": "2025-01-22T07:35:58.554651Z",
     "shell.execute_reply": "2025-01-22T07:35:58.553419Z",
     "shell.execute_reply.started": "2025-01-22T07:35:58.53528Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# if uploaded_file:\n",
    "#     # Save uploaded PDF to a temporary directory\n",
    "#     input_dir_path = \"uploaded_pdfs\"\n",
    "#     with open(f\"{input_dir_path}/document.pdf\", \"wb\") as f:\n",
    "#         f.write(uploaded_file.getbuffer())\n",
    "\n",
    "#     # Load embedding model (replace with actual embedding model)\n",
    "#     embed_model = \"sentence-transformers/all-MiniLM-L6-v2\"  # Example\n",
    "\n",
    "#     if user_query:\n",
    "#         # Generate response\n",
    "#         response = generate_response_from_docs(input_dir_path, user_query, embed_model)\n",
    "\n",
    "#         # Display response\n",
    "#         st.subheader(\"LLM Response:\")\n",
    "#         st.write(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-01-22T07:35:58.556588Z",
     "iopub.status.busy": "2025-01-22T07:35:58.556135Z",
     "iopub.status.idle": "2025-01-22T07:35:58.687268Z",
     "shell.execute_reply": "2025-01-22T07:35:58.685914Z",
     "shell.execute_reply.started": "2025-01-22T07:35:58.556526Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !streamlit run app.py --server.port 8501 & cloudflared tunnel run --token <TOKEN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-01-22T07:35:58.68879Z",
     "iopub.status.busy": "2025-01-22T07:35:58.688455Z",
     "iopub.status.idle": "2025-01-22T07:36:03.381346Z",
     "shell.execute_reply": "2025-01-22T07:36:03.380068Z",
     "shell.execute_reply.started": "2025-01-22T07:35:58.688751Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pip install opencv-python numpy pytesseract pdf2image Pillow transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-01-22T07:36:03.382962Z",
     "iopub.status.busy": "2025-01-22T07:36:03.382597Z",
     "iopub.status.idle": "2025-01-22T07:36:03.387433Z",
     "shell.execute_reply": "2025-01-22T07:36:03.386514Z",
     "shell.execute_reply.started": "2025-01-22T07:36:03.38293Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !wget https://github.com/Baskar-forever/TableExtractor-Advanced-PDF-Table-Extraction/archive/refs/heads/main.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-01-22T07:36:03.389172Z",
     "iopub.status.busy": "2025-01-22T07:36:03.38874Z",
     "iopub.status.idle": "2025-01-22T07:36:03.407301Z",
     "shell.execute_reply": "2025-01-22T07:36:03.40621Z",
     "shell.execute_reply.started": "2025-01-22T07:36:03.389129Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#!rm /kaggle/working/TableExtractor-Advanced-PDF-Table-Extraction-main\n",
    "#!rm -rf /kaggle/working/TableExtractor-Advanced-PDF-Table-Extraction-main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-01-22T07:36:03.409035Z",
     "iopub.status.busy": "2025-01-22T07:36:03.408635Z",
     "iopub.status.idle": "2025-01-22T07:36:03.427346Z",
     "shell.execute_reply": "2025-01-22T07:36:03.426244Z",
     "shell.execute_reply.started": "2025-01-22T07:36:03.409001Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !wget -O /kaggle/working/utils.py https://raw.githubusercontent.com/Baskar-forever/TableExtractor-Advanced-PDF-Table-Extraction/main/utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-01-22T07:36:03.429277Z",
     "iopub.status.busy": "2025-01-22T07:36:03.428808Z",
     "iopub.status.idle": "2025-01-22T07:36:03.44529Z",
     "shell.execute_reply": "2025-01-22T07:36:03.444016Z",
     "shell.execute_reply.started": "2025-01-22T07:36:03.429228Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !head /kaggle/working/utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-01-22T07:36:03.44701Z",
     "iopub.status.busy": "2025-01-22T07:36:03.446542Z",
     "iopub.status.idle": "2025-01-22T07:36:03.464231Z",
     "shell.execute_reply": "2025-01-22T07:36:03.462822Z",
     "shell.execute_reply.started": "2025-01-22T07:36:03.446974Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#!unzip main.zip -d /kaggle/working/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-01-22T07:36:03.46618Z",
     "iopub.status.busy": "2025-01-22T07:36:03.46573Z",
     "iopub.status.idle": "2025-01-22T07:36:03.48195Z",
     "shell.execute_reply": "2025-01-22T07:36:03.480896Z",
     "shell.execute_reply.started": "2025-01-22T07:36:03.466136Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !ls /kaggle/working/\n",
    "# %cd /kaggle/working/TableExtractor-Advanced-PDF-Table-Extraction-main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-01-22T07:36:03.483678Z",
     "iopub.status.busy": "2025-01-22T07:36:03.483218Z",
     "iopub.status.idle": "2025-01-22T07:36:03.498421Z",
     "shell.execute_reply": "2025-01-22T07:36:03.497359Z",
     "shell.execute_reply.started": "2025-01-22T07:36:03.483634Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "\n",
    "# # Add the path to the directory containing utils.py\n",
    "# sys.path.append('/kaggle/working')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-01-22T07:36:03.500223Z",
     "iopub.status.busy": "2025-01-22T07:36:03.499777Z",
     "iopub.status.idle": "2025-01-22T07:36:03.5141Z",
     "shell.execute_reply": "2025-01-22T07:36:03.512948Z",
     "shell.execute_reply.started": "2025-01-22T07:36:03.500177Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !pip install -U langchain langchain_huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-01-22T07:36:03.515637Z",
     "iopub.status.busy": "2025-01-22T07:36:03.515206Z",
     "iopub.status.idle": "2025-01-22T07:36:03.531378Z",
     "shell.execute_reply": "2025-01-22T07:36:03.530232Z",
     "shell.execute_reply.started": "2025-01-22T07:36:03.5156Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from langchain_huggingface import HuggingFaceEndpoint\n",
    "# llm = HuggingFaceEndpoint(repo_id=\"meta-llama/Llama-3.2-3B-Instruct\")\n",
    "# print(\"E\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-01-22T07:36:03.533053Z",
     "iopub.status.busy": "2025-01-22T07:36:03.532594Z",
     "iopub.status.idle": "2025-01-22T07:36:03.551813Z",
     "shell.execute_reply": "2025-01-22T07:36:03.550783Z",
     "shell.execute_reply.started": "2025-01-22T07:36:03.53301Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from utils import TableExtractor\n",
    "\n",
    "# pdf_path = \"/kaggle/input/pdftable/tablePDFtesting.pdf\"\n",
    "# extractor = TableExtractor(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-01-22T07:36:03.553241Z",
     "iopub.status.busy": "2025-01-22T07:36:03.552937Z",
     "iopub.status.idle": "2025-01-22T07:36:03.569156Z",
     "shell.execute_reply": "2025-01-22T07:36:03.568095Z",
     "shell.execute_reply.started": "2025-01-22T07:36:03.553213Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !apt-get install -y poppler-utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-01-22T07:36:03.570689Z",
     "iopub.status.busy": "2025-01-22T07:36:03.570385Z",
     "iopub.status.idle": "2025-01-22T07:36:03.585678Z",
     "shell.execute_reply": "2025-01-22T07:36:03.584502Z",
     "shell.execute_reply.started": "2025-01-22T07:36:03.570662Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from langchain_huggingface import HuggingFaceEndpoint\n",
    "\n",
    "# # Initialize the Llama model (check if this model is correctly loaded in Hugging Face Hub)\n",
    "# llm = HuggingFaceEndpoint(repo_id=\"meta-llama/Llama-3.2-3B-Instruct\")\n",
    "\n",
    "# result=extractor.key_value_pair()\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-01-23T15:03:56.933689Z",
     "iopub.status.busy": "2025-01-23T15:03:56.933373Z",
     "iopub.status.idle": "2025-01-23T15:04:02.967355Z",
     "shell.execute_reply": "2025-01-23T15:04:02.96582Z",
     "shell.execute_reply.started": "2025-01-23T15:03:56.93366Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install easyocr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import easyocr\n",
    "\n",
    "# Initialize reader\n",
    "reader = easyocr.Reader(['en'])\n",
    "\n",
    "# Extract text from image\n",
    "result = reader.readtext('/kaggle/input/cheat-sheets-pdf/ai4all1.jpg')\n",
    "for detection in result:\n",
    "    print(detection[1])  #Extractedtext"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6448947,
     "sourceId": 10406754,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6497607,
     "sourceId": 10494417,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6627675,
     "sourceId": 10695714,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30822,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
